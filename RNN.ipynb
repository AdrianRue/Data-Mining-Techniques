{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded CSV file\n",
    "file_path = 'feature_nans_removed.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the unused 'Unnamed: 0' column and ensure no NaN values in 'mood'\n",
    "data = data.drop(columns=['Unnamed: 0']).dropna(subset=['mood'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1014,), numpy.ndarray, (254,), numpy.ndarray), ((1014,), (254,)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to create variable-length sequences\n",
    "def create_variable_sequences(df, max_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df)):\n",
    "        # Calculate start index for the variable-length sequence\n",
    "        start_ix = max(0, i - max_steps + 1)\n",
    "        # Gather input and output parts of the pattern\n",
    "        seq_x, seq_y = df.iloc[start_ix:i+1, 2:].values, df.iloc[i, 3]  # mood is at index 3\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X, dtype=object), np.array(y)\n",
    "\n",
    "# Creating variable-length sequences using up to a 7-day history\n",
    "max_steps = 7\n",
    "X_var, y_var = create_variable_sequences(data, max_steps)\n",
    "\n",
    "# Splitting the dataset into training and testing sets for variable-length sequences\n",
    "X_train_var, X_test_var, y_train_var, y_test_var = train_test_split(X_var, y_var, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes and types of the resulting arrays\n",
    "(X_train_var.shape, type(X_train_var[0]), X_test_var.shape, type(X_test_var[0])), (y_train_var.shape, y_test_var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated y_train_var_updated shape: (1014,)\n",
      "Updated y_test_var_updated shape: (254,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1014,), (254,), array([3, 4, 5, 6, 7, 8, 9]), array([4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting daily mood scores to integer classes\n",
    "data['mood'] = data['mood'].round().astype(int)\n",
    "\n",
    "# Updating the mood targets in the variable-length sequences\n",
    "def update_targets_with_classes(X, y, df):\n",
    "    # Map mood scores to integer classes\n",
    "    mood_classes = df['mood'].round().astype(int).values\n",
    "    \n",
    "    # Ensure only the labels corresponding to the sequences are used\n",
    "    y_updated = []\n",
    "    for i, seq in enumerate(X):\n",
    "        # Get the index of the last item in the sequence for the label\n",
    "        label_index = len(seq) - 1 + i\n",
    "        if label_index < len(mood_classes):\n",
    "            y_updated.append(mood_classes[label_index])\n",
    "    return X, np.array(y_updated)\n",
    "\n",
    "# Apply the updated targets to the previously created variable-length sequences\n",
    "X_train_var, y_train_var_updated = update_targets_with_classes(X_train_var, y_train_var, data)\n",
    "X_test_var, y_test_var_updated = update_targets_with_classes(X_test_var, y_test_var, data)\n",
    "\n",
    "# Verify the updated target shapes and ensure they match the number of sequences\n",
    "print(f\"Updated y_train_var_updated shape: {y_train_var_updated.shape}\")\n",
    "print(f\"Updated y_test_var_updated shape: {y_test_var_updated.shape}\")\n",
    "\n",
    "# Function to replace NaNs with zero in each sequence\n",
    "def replace_nans(X):\n",
    "    return [np.nan_to_num(np.array(seq, dtype=float), nan=0.0) for seq in X]\n",
    "\n",
    "# Replace NaNs in X_train_var and X_test_var\n",
    "X_train_var = replace_nans(X_train_var)\n",
    "X_test_var = replace_nans(X_test_var)\n",
    "\n",
    "# Verify the updated target shapes and unique classes\n",
    "y_train_var_updated.shape, y_test_var_updated.shape, np.unique(y_train_var_updated), np.unique(y_test_var_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_var shape: (1014,)\n",
      "y_train_var_updated shape: (1014,)\n",
      "X_test_var shape: (254,)\n",
      "y_test_var_updated shape: (254,)\n",
      "Epoch 1, Loss: 1.3717297315597534\n",
      "Epoch 2, Loss: 0.9705755114555359\n",
      "Epoch 3, Loss: 1.0675801038742065\n",
      "Epoch 4, Loss: 1.2877821922302246\n",
      "Epoch 5, Loss: 1.4800549745559692\n",
      "Epoch 6, Loss: 1.0224374532699585\n",
      "Epoch 7, Loss: 1.710258960723877\n",
      "Epoch 8, Loss: 1.305217981338501\n",
      "Epoch 9, Loss: 1.160085678100586\n",
      "Epoch 10, Loss: 1.3078067302703857\n",
      "Accuracy: 46.8503937007874%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Setting the number of LSTM units\n",
    "num_lstm_units = 100\n",
    "\n",
    "# Define the RNN Model\n",
    "class MoodRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoodRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=12, hidden_size=num_lstm_units, batch_first=True)\n",
    "        self.classifier = nn.Linear(num_lstm_units, 7)  # Output layer for 7 classes (moods 3 to 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only take the output from the final timestep\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        # Pass the last timestep output to classifier to get the mood prediction\n",
    "        mood_prediction = self.classifier(last_time_step)\n",
    "        return mood_prediction\n",
    "\n",
    "\n",
    "print(f\"X_train_var shape: {np.array([len(x) for x in X_train_var]).shape}\")\n",
    "print(f\"y_train_var_updated shape: {y_train_var_updated.shape}\")\n",
    "\n",
    "print(f\"X_test_var shape: {np.array([len(x) for x in X_test_var]).shape}\")\n",
    "print(f\"y_test_var_updated shape: {y_test_var_updated.shape}\")\n",
    "\n",
    "\n",
    "# Function to create data loaders with padding\n",
    "def create_padded_loader(X, y, batch_size):\n",
    "    # Pad sequences to the same length\n",
    "    X_padded = nn.utils.rnn.pad_sequence([torch.tensor(x, dtype=torch.float32) for x in X], batch_first=True)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long) - 3  # Subtract 3 to map mood 3-9 to 0-6\n",
    "    dataset = TensorDataset(X_padded, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = create_padded_loader(X_train_var, y_train_var_updated, batch_size)\n",
    "test_loader = create_padded_loader(X_test_var, y_test_var_updated, batch_size)\n",
    "\n",
    "# Example of training loop setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MoodRNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Testing loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN in X_train_var: False\n",
      "NaN in y_train_var: False\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in each sequence of X_train_var\n",
    "nan_in_X_train_var = any(np.isnan(np.array(seq, dtype=float)).any() for seq in X_train_var)\n",
    "print(\"NaN in X_train_var:\", nan_in_X_train_var)\n",
    "\n",
    "# Since y_train_var_updated should be a standard numpy array, checking for NaNs directly should work\n",
    "nan_in_y_train_var = np.isnan(y_train_var_updated).any()\n",
    "print(\"NaN in y_train_var:\", nan_in_y_train_var)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
