{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('feature_nans_removed.csv')\n",
    "data = data.drop(columns=['Unnamed: 0']).dropna(subset=['mood'])\n",
    "\n",
    "# Function to categorize mood into 5 bins\n",
    "def categorize_mood(mood_scores):\n",
    "    bins = [2.5, 4.5, 6.5, 7.5, 8.5, 10] # Adjust bins?\n",
    "    return pd.cut(mood_scores, bins=bins, labels=[0, 1, 2, 3, 4], right=False)\n",
    "\n",
    "# Apply categorization\n",
    "data['mood'] = categorize_mood(data['mood'])\n",
    "\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to create variable-length sequences\n",
    "def create_variable_sequences(df, max_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df)):\n",
    "        # Calculate start index for the variable-length sequence\n",
    "        start_ix = max(0, i - max_steps + 1)\n",
    "        # Gather input and output parts of the pattern\n",
    "        seq_x, seq_y = df.iloc[start_ix:i+1, 2:].values, df.iloc[i, 3]  # mood is at index 3\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X, dtype=object), np.array(y)\n",
    "\n",
    "# Creating variable-length sequences using up to a 7-day history\n",
    "max_steps = 7\n",
    "X_var, y_var = create_variable_sequences(data, max_steps)\n",
    "\n",
    "# Splitting the dataset into training and testing sets for variable-length sequences\n",
    "X_train_var, X_test_var, y_train_var, y_test_var = train_test_split(X_var, y_var, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to remove sequences containing NaNs\n",
    "def remove_sequences_with_nans(sequences):\n",
    "    return [seq for seq in sequences if not np.isnan(np.array(seq)).any()]\n",
    "\n",
    "# Remove problematic sequences\n",
    "X_train_var_clean = remove_sequences_with_nans(X_train_var)\n",
    "y_train_var_clean = [y_train_var[i] for i in range(len(X_train_var)) if i not in [17]] \n",
    "\n",
    "X_test_var_clean = remove_sequences_with_nans(X_test_var)\n",
    "y_test_var_clean = [y_test_var[i] for i in range(len(X_test_var)) if i not in [5]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated y_train_var_updated shape: (918,)\n",
      "Updated y_test_var_updated shape: (230,)\n",
      "Training labels shape and unique classes: (918,), [0 1 2 3 4]\n",
      "Testing labels shape and unique classes: (230,), [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the update_targets_with_classes function uses the existing categorical labels directly\n",
    "def update_targets_with_classes(X, y, df):\n",
    "    # Mood classes are already categorized as integers (labels 0 to 4)\n",
    "    mood_classes = df['mood'].values\n",
    "    \n",
    "    # Ensure only the labels corresponding to the sequences are used\n",
    "    y_updated = []\n",
    "    for i, seq in enumerate(X):\n",
    "        # Get the index of the last item in the sequence for the label\n",
    "        label_index = len(seq) - 1 + i\n",
    "        if label_index < len(mood_classes):\n",
    "            y_updated.append(mood_classes[label_index])\n",
    "    return X, np.array(y_updated)\n",
    "\n",
    "# Apply the updated targets to the previously created variable-length sequences\n",
    "X_train_var, y_train_var_updated = update_targets_with_classes(X_train_var, y_train_var, data)\n",
    "X_test_var, y_test_var_updated = update_targets_with_classes(X_test_var, y_test_var, data)\n",
    "\n",
    "# Print updated shapes of targets and check for consistency and correct mappings\n",
    "print(f\"Updated y_train_var_updated shape: {y_train_var_updated.shape}\")\n",
    "print(f\"Updated y_test_var_updated shape: {y_test_var_updated.shape}\")\n",
    "\n",
    "# Function to replace NaNs with zero in each sequence, which is already defined, so applying it here\n",
    "X_train_var_clean = replace_nans(X_train_var)\n",
    "X_test_var_clean = replace_nans(X_test_var)\n",
    "\n",
    "# Check the shapes and unique classes in the updated target arrays to ensure correct application\n",
    "print(f\"Training labels shape and unique classes: {y_train_var_updated.shape}, {np.unique(y_train_var_updated)}\")\n",
    "print(f\"Testing labels shape and unique classes: {y_test_var_updated.shape}, {np.unique(y_test_var_updated)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_var shape: (918,)\n",
      "y_train_var_updated shape: (918,)\n",
      "X_test_var shape: (230,)\n",
      "y_test_var_updated shape: (230,)\n",
      "Epoch 1, Loss: 1.401260256767273\n",
      "Epoch 2, Loss: 1.3616002798080444\n",
      "Epoch 3, Loss: 1.587344765663147\n",
      "Epoch 4, Loss: 1.4762881994247437\n",
      "Epoch 5, Loss: 1.3855551481246948\n",
      "Epoch 6, Loss: 1.4724159240722656\n",
      "Epoch 7, Loss: 1.4539262056350708\n",
      "Epoch 8, Loss: 1.5380122661590576\n",
      "Epoch 9, Loss: 1.4402896165847778\n",
      "Epoch 10, Loss: 1.2989252805709839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Mood 0       0.00      0.00      0.00         4\n",
      "      Mood 1       0.00      0.00      0.00        35\n",
      "      Mood 2       0.51      0.70      0.59       122\n",
      "      Mood 3       0.17      0.01      0.03        68\n",
      "      Mood 4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.37       230\n",
      "   macro avg       0.13      0.14      0.12       230\n",
      "weighted avg       0.32      0.37      0.32       230\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koole/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/koole/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/koole/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Setting the number of LSTM units\n",
    "num_lstm_units = 100\n",
    "\n",
    "# Define the RNN Model\n",
    "class MoodRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoodRNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=12, hidden_size=num_lstm_units, batch_first=True)\n",
    "        self.classifier = nn.Linear(num_lstm_units, 5)  # Output layer for 5 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Only take the output from the final timestep\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        # Pass the last timestep output to classifier to get the mood prediction\n",
    "        mood_prediction = self.classifier(last_time_step)\n",
    "        return mood_prediction\n",
    "\n",
    "# Print array shapes to confirm correct setup\n",
    "print(f\"X_train_var shape: {np.array([len(x) for x in X_train_var]).shape}\")\n",
    "print(f\"y_train_var_updated shape: {y_train_var_updated.shape}\")\n",
    "print(f\"X_test_var shape: {np.array([len(x) for x in X_test_var]).shape}\")\n",
    "print(f\"y_test_var_updated shape: {y_test_var_updated.shape}\")\n",
    "\n",
    "# Function to create data loaders with padding\n",
    "def create_padded_loader(X, y, batch_size):\n",
    "    # Pad sequences to the same length\n",
    "    X_padded = nn.utils.rnn.pad_sequence([torch.tensor(x, dtype=torch.float32) for x in X], batch_first=True)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    dataset = TensorDataset(X_padded, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = create_padded_loader(X_train_var, y_train_var_updated, batch_size)\n",
    "test_loader = create_padded_loader(X_test_var, y_test_var_updated, batch_size)\n",
    "\n",
    "# Example of training loop setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MoodRNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Re-initialize model to reset any corrupted weights\n",
    "model = MoodRNN().to(device)\n",
    "\n",
    "# Adding weight initialization\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        nn.init.constant_(param, 0.0)\n",
    "    elif 'weight' in name:\n",
    "        nn.init.xavier_uniform_(param)\n",
    "\n",
    "# Training loop with additional diagnostics\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"NaN loss detected at epoch {epoch+1}\")\n",
    "            break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "\n",
    "# Evaluate model again after adjustments\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(all_labels, all_predictions, target_names=[f'Mood {i}' for i in range(5)]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
